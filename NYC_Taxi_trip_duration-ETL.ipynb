{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.unmount(\"/mnt/blobstorage\")\n",
    "\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.volstore.blob.core.windows.net\",\n",
    "    \"dn7fGFQ+q2yNtue6G3i+6uGPAwPPAZDpee7lXrRn6vkacMMrjum9D3tlN5gGdXMpBm3PZuhVPYYi+AStZ/5ZRg==\"\n",
    ")\n",
    "\n",
    "# Define the storage account details\n",
    "storage_account_name = \"volstore\"\n",
    "container_name = \"datastore\"\n",
    "storage_account_key = \"dn7fGFQ+q2yNtue6G3i+6uGPAwPPAZDpee7lXrRn6vkacMMrjum9D3tlN5gGdXMpBm3PZuhVPYYi+AStZ/5ZRg==\"\n",
    "\n",
    "# Mount the Blob Storage container\n",
    "dbutils.fs.mount(\n",
    "    source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "    mount_point = \"/mnt/blobstorage\",\n",
    "    extra_configs = {f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": storage_account_key}\n",
    ")\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"/mnt/blobstorage/Dataset/2014/nyc_taxi_data_2014/nyc_taxi_data_2014.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217863e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import pipeline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data in the dataset\n",
    "df.show(5)\n",
    "\n",
    "# Check the number of rows\n",
    "num_rows = df.count()\n",
    "\n",
    "# Check the number of columns\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "# Display the shape\n",
    "print(f\"Shape of the DataFrame: ({num_rows}, {num_columns})\")\n",
    "\n",
    "# Identify and drop duplicate rows\n",
    "df_no_duplicates = df.dropDuplicates()\n",
    "\n",
    "# Count the number of null values for each column\n",
    "null_counts = df.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "\n",
    "#Only keep trips that lasted less than 5900 seconds\n",
    "df = df[(df.trip_distance < 5900)]\n",
    "\n",
    "#Only keep trips with passengers\n",
    "df = df[(df.passenger_count > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187460d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deal with categorical features \n",
    "\n",
    "# Indexing the categorical column\n",
    "indexer = StringIndexer(inputCol='store_and_fwd_flag', outputCol='store_and_fwd_flag_indexed')\n",
    "\n",
    "# One-hot encoding the indexed column\n",
    "encoder = OneHotEncoder(inputCol='store_and_fwd_flag_indexed', outputCol='store_and_fwd_flag_encoded')\n",
    "\n",
    "# Creating a pipeline to execute both operations\n",
    "pipeline = Pipeline(stages=[indexer, encoder])\n",
    "\n",
    "# Applying the pipeline to the DataFrame\n",
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Dropping the original categorical column and the indexed one\n",
    "df_transformed = df_transformed.drop('store_and_fwd_flag', 'store_and_fwd_flag_indexed')\n",
    "\n",
    "# One-hot encoding binary categorical features\n",
    "indexer = StringIndexer(inputCol='vendor_id', outputCol='vendor_id_indexed')\n",
    "encoder = OneHotEncoder(inputCol='vendor_id_indexed', outputCol='vendor_id_encoded')\n",
    "\n",
    "# Creating a pipeline to execute both operations\n",
    "pipeline = Pipeline(stages=[indexer, encoder])\n",
    "\n",
    "# Applying the pipeline to the DataFrame\n",
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "\n",
    "# Dropping the original categorical column and the indexed one\n",
    "df_transformed = df_transformed.drop('vendor_id', 'vendor_id_indexed')\n",
    "\n",
    "#Datetyping the dates\n",
    "df['pickup_datetime'] = pd.to_datetime(df.pickup_datetime)\n",
    "\n",
    "df.drop(['dropoff_datetime'], axis=1, inplace=True) #as we don't have this feature in the testset\n",
    "\n",
    "#Date features creations and deletions\n",
    "df['month'] = df.pickup_datetime.dt.month\n",
    "df['week'] = df.pickup_datetime.dt.week\n",
    "df['weekday'] = df.pickup_datetime.dt.weekday\n",
    "df['hour'] = df.pickup_datetime.dt.hour\n",
    "df['minute'] = df.pickup_datetime.dt.minute\n",
    "df['minute_oftheday'] = df['hour'] * 60 + df['minute']\n",
    "df.drop(['minute'], axis=1, inplace=True)\n",
    "\n",
    "df.drop(['pickup_datetime'], axis=1, inplace=True)\n",
    "\n",
    "df.info()\n",
    "\n",
    "\n",
    "#Function aiming at calculating distances from coordinates\n",
    "def ft_haversine_distance(lat1, lng1, lat2, lng2):\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371 #km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "#Add distance feature\n",
    "df['distance'] = ft_haversine_distance(df['pickup_latitude'].values,\n",
    "                                                 df['pickup_longitude'].values, \n",
    "                                                 df['dropoff_latitude'].values,\n",
    "                                                 df['dropoff_longitude'].values)\n",
    "\n",
    "#Function aiming at calculating the direction\n",
    "def ft_degree(lat1, lng1, lat2, lng2):\n",
    "    AVG_EARTH_RADIUS = 6371 #km\n",
    "    lng_delta_rad = np.radians(lng2 - lng1)\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n",
    "    return np.degrees(np.arctan2(y, x))\n",
    "\n",
    "#Add direction feature\n",
    "df['direction'] = ft_degree(df['pickup_latitude'].values,\n",
    "                                df['pickup_longitude'].values,\n",
    "                                df['dropoff_latitude'].values,\n",
    "                                df['dropoff_longitude'].values)\n",
    "\n",
    "\n",
    "#Remove distance outliers\n",
    "df = df[(df.distance < 200)]\n",
    "\n",
    "#Create speed feature\n",
    "df['speed'] = df.distance / df.trip_duration\n",
    "\n",
    "#Remove speed outliers\n",
    "df = df[(df.speed < 30)]\n",
    "df.drop(['speed'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f304a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting outlier trips (outside of NYC) and remove them\n",
    "\n",
    "# lat and long number comes from & credit to DrGuillermo: Animation\n",
    "xlim = [-74.03, -73.77]\n",
    "ylim = [40.63, 40.85]\n",
    "train = df[(df.pickup_longitude> xlim[0]) & (df.pickup_longitude < xlim[1])]\n",
    "train = df[(train.dropoff_longitude> xlim[0]) & (df.dropoff_longitude < xlim[1])]\n",
    "train = df[(train.pickup_latitude> ylim[0]) & (df.pickup_latitude < ylim[1])]\n",
    "train = df[(train.dropoff_latitude> ylim[0]) & (df.dropoff_latitude < ylim[1])]\n",
    "plt.plot(df['pickup_longitude'], train['pickup_latitude'], '.', color='k', alpha=0.8)\n",
    "plt.title('Pickup Location Lat and Long', weight = 'bold')\n",
    "\n",
    "\n",
    "# only select useful columns\n",
    "subset_train = train[['trip_distance', 'fare_amount','vendor_id','mta_tax','passenger_count','hour']]\n",
    "log_duration = np.log1p(subset_train['trip_duration'])\n",
    "subset_train.sample(False, 0.1, seed=42).show(5)\n",
    "\n",
    "# Avg Trip Durations by Weekday\n",
    "weekday_list = ['Mon','Tues','Wed','Thurs','Fri','Sat','Sun']\n",
    "g = sns.factorplot(kind='bar',        # Boxplot\n",
    "               y='trip_duration',       # Y-axis - values for boxplot\n",
    "               x='weekday',        # X-axis - first factor\n",
    "               #estimator = np.sum, \n",
    "               data=subset_train,        # Dataframe \n",
    "               size=6,            # Figure size (x100px)      \n",
    "               aspect=1.6,        # Width = size * aspect \n",
    "               order = list(weekday_list),\n",
    "               legend_out=False) \n",
    "plt.title('Avg Trip Durations by Weekday\\n', weight = 'bold', size = 20)\n",
    "plt.xlabel('Weekday', size = 18,weight = 'bold')\n",
    "plt.ylabel('Average trip duration', size = 18,weight = 'bold')\n",
    "g.set_xticklabels(rotation=45)\n",
    "\n",
    "# Total Distance (in miles) by Weekday\n",
    "weekday_list = ['Mon','Tues','Wed','Thurs','Fri','Sat','Sun']\n",
    "g = sns.factorplot(kind='bar',        # Boxplot\n",
    "               y='haversine_distance',       # Y-axis - values for boxplot\n",
    "               x='weekday',        # X-axis - first factor\n",
    "               estimator = np.sum, \n",
    "               data=subset_train,        # Dataframe \n",
    "               size=6,            # Figure size (x100px)      \n",
    "               aspect=1.6,        # Width = size * aspect \n",
    "               order = list(weekday_list),\n",
    "               legend_out=False) \n",
    "plt.title('Total Distance (in miles) by Weekday\\n', weight = 'bold', size = 20)\n",
    "plt.xlabel('Weekday', size = 18,weight = 'bold')\n",
    "plt.ylabel('Total Distance (in miles) ', size = 18,weight = 'bold')\n",
    "g.set_xticklabels(rotation=45)\n",
    "\n",
    "#Average Duration by Hour of Day and Day of Week\n",
    "sns.set(font_scale=1.3)\n",
    "g = sns.factorplot('pickup_hour', \n",
    "                   'trip_duration', \n",
    "                   hue = 'weekday', \n",
    "                   estimator = np.mean, \n",
    "                   data = subset_train, \n",
    "                   size = 8, \n",
    "                   aspect = 2, \n",
    "                    ci=None,\n",
    "                   legend_out=False)\n",
    "sns.plt.title('Average Duration by Hour of Day and Day of Week \\n',weight='bold', size = 20)\n",
    "plt.xlabel('start hour', size = 18,weight = 'bold')\n",
    "plt.ylabel('avg duration', size = 18,weight = 'bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a757a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour']=subset_train.pickup_hour.astype(str)\n",
    "df = df[['log_duration','log_haversine_distance', 'weekday','month','hour']]\n",
    "\n",
    "X = df.drop(\"log_duration\",axis=1)\n",
    "y = df[\"log_duration\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models\n",
    "\n",
    "# GradientBoosting\n",
    "gb = GradientBoostingRegressor()\n",
    "gb.fit(X_train, y_train)\n",
    "#print(gb.score(X_train, y_train), gb.score(X_test, y_test))\n",
    "print(\"classic gradient boosting\")\n",
    "print(np.sqrt(mean_squared_error(y_test, np.clip(gb.predict(X_test),0,None))))\n",
    "\n",
    "# RandomForest\n",
    "rfm = RandomForestRegressor(bootstrap=True,max_depth=90,max_features='auto',min_samples_split=15,min_samples_leaf=10,n_estimators=30)\n",
    "rfm.fit(X_train, y_train)\n",
    "print(\"random forest\")\n",
    "print(rfm.score(X_train, y_train), rfm.score(X_test, y_test),rfm.score(y_test, log_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
